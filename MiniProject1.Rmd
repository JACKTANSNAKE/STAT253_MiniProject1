---
title: "Mini-Project 1"
author: Jack Tan, Debbie Sun, Alex Denzler
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r message = FALSE}
library(ggplot2)
library(dplyr)
library(caret)
library(RANN)
library(stringr)
```

\
\
## Part 1: Ready the data
```{r, warning=FALSE}
airbnb <- read.csv("https://www.macalester.edu/~ajohns24/data/NYC_airbnb_kaggle.csv")
nbhd <- read.csv("https://www.macalester.edu/~ajohns24/data/NYC_nbhd_kaggle.csv")
airbnb_complete <- left_join(nbhd, airbnb, by = c("neighbourhood" = "neighbourhood_cleansed"))
dim(airbnb_complete)
```

### One example of correlation(catergorical)
```{r}
ggplot(airbnb_complete, aes(x = host_is_superhost, y = price)) + 
  geom_boxplot()
```

### Deal with the data
```{r message = FALSE, warning= FALSE, cache=TRUE}
airbnb_complete <- airbnb_complete %>% 
  filter(price < 1000) %>%
  filter(host_response_rate != "") %>%
  mutate(amenity_count = str_count(amenities, ',') + 1) %>%
  #filter(host_is_superhost != "") %>%
  #filter(host_has_profile_pic != "") %>%
  #filter(host_response_time != "") %>%
  #filter(require_guest_profile_picture != "")
  select(-id,-longitude,-latitude,-amenities,-square_feet,-calendar_updated,-neighbourhood)
dim(airbnb_complete)
max(airbnb_complete$amenity_count)
```

The `host_response_rate` variable initially has 4 levels, "T", "F", "NA", and "". We found similar situations in variables: `host_is_superhost`, `host_has_profile_pic`, `host_response_time`, and `require_guest_profile_picture`. We tried to filter out all data which have no information of these variables, since it's hard for us to interpret "" information as we do not know what it stands for and catergorical variables cannot be imputed by the KNN methods. Surprisingly, the dimension of the dataset is the same with all five variables filtered of "" compared to if we only filter "host_response_rate" of ""; therefore, we assumed that the "" data in all five variables were the caused by technical issues when collecting the data. Thus, we think it is reasonable and easier for later processing the data that we filter out the "" data points. For simplicity, we just filtered `host_response_rate` so that we can eliminate all missing information in these predictors. 

```{r}
ggplot(airbnb_complete, aes(x = host_is_superhost, y = price)) + 
  geom_boxplot()
```

After filtering the blank data ""

We also deselected `id` and `calendar_updated` because we think they have no influence on the price. Also, we dropped `longitude` and `latitude` because the data of each is very similar to itself since it's in the same city and the `neighboorhood_group` variable may better catch the trend then these two variables. Same reason with deselecting `neighboorhood`(although running with `neighbourhood` in LASSO resulted in a model with $R^2$ of 0.68). We also deselected `amenities` since there are maximum 87 catergories of amenities so we would technically get $\sum_{k=1}^{87} {87 \choose k} - 1$  different reference levels for this single variable and make the resulting model overly complicated and not interpretable. However, we believe that might exist a relationship between the number of amenities an airbnb has and its price, since intuitively pricey airbnbs should have more services. Therefore, we counted the number of `amenities`, as `amenity_count`, in the dataset and dropped amenities.

### Randomly choose 5000 data points
```{r,cache=TRUE}
set.seed(253)
airbnb_5000 <- sample_n(airbnb_complete,5000)
dim(airbnb_5000)
```

\
\
\
\
\
\



## Part 2: Analyze
### impute with KNN
```{r,cache=TRUE}
impute_info <- airbnb_5000 %>%
  select(-price,-amenity_count) %>%
  preProcess(method = "knnImpute")
airbnb_5000 <- predict(impute_info, newdata = airbnb_5000)
```

### Modeling with LASSO
```{r,cache=FALSE,warning = FALSE}
lambda_grid <- 10^seq(-3, 1, length = 100)

# Set the seed 
    set.seed(253)
    
    lasso_model_origin <- train(
      price ~ .,
      data = airbnb_5000,
      method = "glmnet",
      trControl = trainControl(method = "cv", number = 10, selectionFunction = "oneSE"),
      tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
      metric = "MAE",
      na.action = na.omit
    )
    
lasso_model_origin$results %>%
  filter(lambda == lasso_model_origin$bestTune$lambda)

# Combine residuals & predictions into data frame
result_df <- data.frame(resid = resid(lasso_model_origin), fitted = fitted(lasso_model_origin))

# Residual plot
ggplot(result_df, aes(x = fitted, y = resid)) + 
    geom_point() + 
    geom_hline(yintercept = 0)
```

We try with lasso model and choose the optimal one within one standard deviation to the best model in lasso algorithm so that we don't have to complicate our model with a similar $R^2$. The $R^2$ of the lasso model is 0.5943, indicating that our model is moderately strong since almost 60 percent of the variability in price is explained by lasso model. Also, the residual plot is balanced on both sides of the x axis, but the residuals are clustered closer to x-axis in the range of 4.5 to 5. 



### Modeling with LASSO with log(price)
```{r, cache=TRUE}
airbnb_5000$price <- log(airbnb_5000$price + 1)
```

```{r,cache=FALSE,warning = FALSE}
lambda_grid <- 10^seq(-3, 1, length = 100)

# Set the seed 
    set.seed(253)
    
    lasso_model <- train(
      price ~ .,
      data = airbnb_5000,
      method = "glmnet",
      trControl = trainControl(method = "cv", number = 10, selectionFunction = "oneSE"),
      tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
      metric = "MAE",
      na.action = na.omit
    )
    
lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda)

```


```{r message = FALSE,cache=TRUE}
model_coef <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda)
model_coef 
#It's a super long list of coeffs and we do not want them listed up in the report.

model_coef
```

```{r message = FALSE, cache = TRUE}
lasso_model$bestTune$lambda
# Codebook for which variables the numbers correspond to

plot(lasso_model,xlim = c(0,0.025), ylim = c(0.3,0.32))

# Combine residuals & predictions into data frame
result_df <- data.frame(resid = resid(lasso_model), fitted = fitted(lasso_model))

 # Residual plot
ggplot(result_df, aes(x = fitted, y = resid)) + 
    geom_point() + 
    geom_hline(yintercept = 0)
```

### Deselecting the variables considered insignificant by Lasso
```{r message = FALSE,cache=TRUE}
airbnb_5000_complete <- airbnb_5000 %>% 
  select(-host_response_rate,-host_is_superhost,-host_has_profile_pic,-is_location_exact,-property_type,-beds,-bed_type,-maximum_nights,-number_of_reviews,-cancellation_policy,-require_guest_profile_picture)
dim(airbnb_5000_complete)
```

### Least Square model to compare
```{r, cache=TRUE}
ls_model <- train(
    price ~ .,
    data = airbnb_5000_complete,
    method = "lm",
    trControl = trainControl(method = "cv", number = 10),
    na.action = na.omit
)

summary(ls_model)
ls_model$results$Rsquared
ls_model$resample%>%
  summarize(mean(MAE))
result_df_ls <- data.frame(resid = resid(ls_model), fitted = fitted(ls_model))

 # Residual plot
ggplot(result_df_ls, aes(x = fitted, y = resid)) + 
    geom_point() + 
    geom_hline(yintercept = 0)
```


The $R^2$ of least squared model is 0.598 which is similar to the lasso model. The least squares model is made up of only the coefficients that the LASSO deemed to be significant. Therefore, we are simply making the LASSO model more interpretable by turning it into a least squares model. The residuals for the least squared model shows the similar trend for the LASSO model.


### Removing the only catergorical variable that has problem with GAM
```{r,cache=TRUE,eval = fALSE}
airbnb_5000_complete <- airbnb_5000_complete %>% select(-room_type, -host_response_time)
```

Originally, the GAM algorithm does not like our airbnb_complete dataset. We thought there might be too many catergorical variables and it can't handle that much. So we started out removing multiple less significant variables out (according to the least square model). After trials and errors, we found that the only variable that had problem with GAM was room_type, so we just removed it.(Explanation?).

### Building the GAM model
```{r warning = FALSE, message = FALSE, fig.width = 8, fig.height = 12,eval = FALSE}
 # Set the seed
    set.seed(253)

 # Run the GAM
    gam_model <- train(
      price ~.,
      data = airbnb_5000_complete,
      method = "gamLoess",
      tuneGrid = data.frame(span = seq(0.1,1,length = 30), degree = 1),
      trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
      metric = "MAE",
      na.action = na.omit
    )
```

### Showing the GAM results
```{r fig.width = 8, fig.height =12,eval = FALSE}
    # Calculate the CV MAE of the best model
    gam_model$results %>% 
      filter(span == gam_model$bestTune$span)
    gam_model$resample %>% 
      summarize(mean(MAE))
    
    par(mfrow = c(5,4))
    
    # Make plots
    plot(gam_model$finalModel)
```


The $R^2$ in GAM model that we tried is 0.496 and its MAE is relatively bigger; therefore, we decide to not to use in our analysis.



### Contrast with KNN
```{r, eval = FALSE}
set.seed(253)

# Run the KNN
knn_model <- train(
  price ~ .,
  data = airbnb_5000_complete,
  preProcess = c("center","scale"),
  method = "knn",
  tuneGrid = data.frame(k = c(1:19, seq(20, 100, by = 5), seq(150, 450, by = 50))),
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
  metric = "MAE",
  na.action = na.omit
)
```

### Result of KNN
```{r, eval = FALSE}
knn_model$bestTune
knn_model$results %>% 
  filter(k == knn_model$bestTune$k)
```

Same thing with the KNN model, it lacks interpretability and its $R^2$ is just slightly better LASSO. So we also decided to delete it from our analysis.
\
\
\
\
\



## Part 3: Summarize


price ~ amenity_count
The coefficient of `amenity_count` in the LASSO model is 0.0051755. Thus, as `amenity_count` increases by 1, `price` multiplies by $e^(0.0051755) = 1.005189$. This is intuitive because as listings provide more amenities for guests hosts can increase the price. 

```{r}
mean(airbnb_5000_complete$amenity_count)
max(airbnb_5000_complete$amenity_count)
min(airbnb_5000_complete$amenity_count)
median(airbnb_5000_complete$amenity_count)
airbnb_5000_complete %>% filter(amenity_count == 64)

```






\
\
\
\
\
\



## Part 4: Contributions



